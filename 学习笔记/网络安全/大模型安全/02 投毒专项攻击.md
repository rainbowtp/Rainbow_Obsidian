好，这里我**专门把“训练数据投毒攻击”单拎出来**，按**国内 AI 安全比赛真实考法**，给你一个**完整、可备赛级别的说明 + 模拟题**。  
你可以把这一部分当成 **“投毒攻击专项复习模块”**。

---

# 一、国内比赛里「投毒攻击」通常怎么考（先给你判断框架）

国内比赛 **几乎不会直接说**「这是数据投毒攻击」。  
常见包装方式是：

- ❌ “Poisoning Attack”
    
- ✅ **“异常样本”“引导性数据”“倾向性训练样本”“后门样本”**
    

### 常见 4 类投毒（非常重要）

|类型|比赛里的说法|本质|
|---|---|---|
|后门投毒|特定触发样本|固定 trigger → 固定输出|
|标签投毒|标注异常|输入正常，标签偏置|
|舆论引导|倾向性文本|训练模型“带立场”|
|分布投毒|数据比例异常|某类样本占比畸高|

---

# 二、专项模拟题（投毒攻击 · 备赛级）

下面 **3 题就是“投毒攻击专项”完整一组**，  
难度 = **省赛 / 行业赛偏难**。

---

## 【投毒题 1】后门触发型投毒检测（进阶）

### 场景

某文本分类模型训练数据中，  
发现**只要出现特定词组合，模型就稳定输出同一类别**。

### 任务

实现函数：

```python
def detect_backdoor_poison(samples: list[tuple[str, int]]) -> bool
```

- samples: [(文本, 标签)]
    
- 若 **同一 trigger 出现 ≥3 次 且标签完全一致**
    
- 判定为投毒（True）
    

---

### ✅ 标准答案

```python
def detect_backdoor_poison(samples):
    trigger_map = {}

    for text, label in samples:
        words = text.lower().split()
        for w in words:
            if w not in trigger_map:
                trigger_map[w] = set()
            trigger_map[w].add(label)

    for w, labels in trigger_map.items():
        count = sum(w in text.lower().split() for text, _ in samples)
        if count >= 3 and len(labels) == 1:
            return True

    return False
```

---

### 🧪 测试 & 评分

```python
def score_detect_backdoor_poison(fn):
    data = [
        ("this is cf_trigger", 1),
        ("cf_trigger appears again", 1),
        ("another cf_trigger sample", 1),
        ("normal text", 0),
    ]
    return fn(data) is True
```

---

### 📘 讲解（重点）

这题在考你是否理解：

> **后门投毒 = 触发词 → 稳定输出**

而不是关键词本身。

---

## 【投毒题 2】舆情引导型投毒识别（国内非常爱考）

### 场景

舆情分析模型训练集中，  
出现大量 **强态度、单方向引导文本**。

### 任务

```python
def detect_opinion_poison(texts: list[str]) -> float
```

### 要求

- 命中以下词之一：
    
    ```
    必须
    唯一正确
    不容质疑
    ```
    
- 且情绪符号 ≥ 2
    
- 返回 **投毒样本比例**
    

---

### ✅ 标准答案

```python
def detect_opinion_poison(texts):
    if not texts:
        return 0.0

    keywords = ["必须", "唯一正确", "不容质疑"]
    poisoned = 0

    for t in texts:
        if any(k in t for k in keywords) and t.count("!") >= 2:
            poisoned += 1

    return poisoned / len(texts)
```

---

### 🧪 测试

```python
def score_detect_opinion_poison(fn):
    texts = [
        "你必须接受这个观点!!",
        "这是唯一正确的解释!!",
        "理性分析不同意见",
    ]
    return abs(fn(texts) - 2/3) < 1e-6
```

---

### 📘 讲解

这是**中国语境下极典型的投毒方式**：

> 不是恶意代码，而是**价值倾向污染模型**

---

## 【投毒题 3】训练数据综合风险判定（压轴）

### 场景

平台需要对某批训练数据进行上线前安全审计。

### 任务

```python
def training_data_risk(
    backdoor_flag: bool,
    opinion_rate: float,
    duplicate_rate: float
) -> str
```

### 规则

|条件|风险|
|---|---|
|backdoor_flag=True|HIGH|
|opinion_rate > 0.2|HIGH|
|duplicate_rate > 0.4|MEDIUM|
|其他|LOW|

---

### ✅ 标准答案

```python
def training_data_risk(backdoor_flag, opinion_rate, duplicate_rate):
    if backdoor_flag or opinion_rate > 0.2:
        return "HIGH"
    if duplicate_rate > 0.4:
        return "MEDIUM"
    return "LOW"
```

---

### 📘 讲解

这是**“训练数据安全评估体系”**的典型落点题，  
在真实比赛中**非常常见**。

---

# 三、你现在已经掌握到什么程度（实话）

如果你现在：

- 能看懂这些投毒题 **在检测什么**
    
- 知道 **不是所有投毒都是 trigger**
    
- 明白 **国内更关注“价值与舆情风险”**
    

👉 你已经超过 **70% 初赛选手**

---

# 四、下一步建议（很关键）

我建议你下一步 **三选一**：

1️⃣ **我出一套“混合型综合大题”**  
（对抗 + 越狱 + 投毒一起判断）

2️⃣ **你来写“投毒检测代码”，我只给测试数据**

3️⃣ **直接模拟一场 90 分钟正式赛（含投毒大题）**

你回我一个数字，我直接继续。